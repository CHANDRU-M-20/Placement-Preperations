{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05f8311c",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc1ddc3",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 🔄 NLP Pipeline Overview\n",
    "\n",
    "1. **Text Input**\n",
    "2. **Text Cleaning / Preprocessing**\n",
    "3. **Tokenization**\n",
    "4. **Stopword Removal**\n",
    "5. **Stemming / Lemmatization**\n",
    "6. **Part-of-Speech Tagging**\n",
    "7. **Named Entity Recognition**\n",
    "8. **Vectorization** (for ML)\n",
    "9. **Modeling / Analysis**\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Step-by-Step Pipeline with Code\n",
    "\n",
    "We’ll use **NLTK** for stemming and **spaCy** for lemmatization and advanced tasks.\n",
    "\n",
    "### 🔹 1. Text Input\n",
    "\n",
    "```python\n",
    "text = \"John's running in the marathon was better than his previous attempts. He studies NLP techniques daily.\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 2. Text Cleaning\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "# Remove punctuation and lowercase\n",
    "clean_text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "print(clean_text)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "`johns running in the marathon was better than his previous attempts he studies nlp techniques daily`\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 3. Tokenization\n",
    "\n",
    "```python\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "tokens = word_tokenize(clean_text)\n",
    "print(tokens)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "`['johns', 'running', 'in', 'the', 'marathon', 'was', 'better', 'than', 'his', 'previous', 'attempts', 'he', 'studies', 'nlp', 'techniques', 'daily']`\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 4. Stopword Removal\n",
    "\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "print(filtered_tokens)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "`['johns', 'running', 'marathon', 'better', 'previous', 'attempts', 'studies', 'nlp', 'techniques', 'daily']`\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 5. Stemming (using NLTK)\n",
    "\n",
    "```python\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(stemmed)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "`['john', 'run', 'marathon', 'better', 'previou', 'attempt', 'studi', 'nlp', 'techniqu', 'daili']`\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 6. Lemmatization (using spaCy)\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\" \".join(filtered_tokens))\n",
    "lemmatized = [token.lemma_ for token in doc]\n",
    "print(lemmatized)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "`['john', 'run', 'marathon', 'well', 'previous', 'attempt', 'study', 'NLP', 'technique', 'daily']`\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 7. POS Tagging\n",
    "\n",
    "```python\n",
    "for token in doc:\n",
    "    print((token.text, token.pos_))\n",
    "```\n",
    "\n",
    "**Example Output**:\n",
    "\n",
    "```\n",
    "('john', 'PROPN')\n",
    "('run', 'VERB')\n",
    "('marathon', 'NOUN')\n",
    "('well', 'ADV')\n",
    "...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 8. Named Entity Recognition (NER)\n",
    "\n",
    "```python\n",
    "doc = nlp(text)  # use original text\n",
    "for ent in doc.ents:\n",
    "    print((ent.text, ent.label_))\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "\n",
    "```\n",
    "('John', 'PERSON')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 9. Vectorization (Optional for ML)\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\"John is running a marathon.\", \"He studies NLP.\"]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Summary\n",
    "\n",
    "| Stage            | Tool Used    | Output                             |\n",
    "| ---------------- | ------------ | ---------------------------------- |\n",
    "| Cleaning         | Regex        | Clean, lowercase text              |\n",
    "| Tokenization     | NLTK / spaCy | List of words                      |\n",
    "| Stopword Removal | NLTK         | Filtered tokens                    |\n",
    "| Stemming         | NLTK         | Root-like forms (may be non-words) |\n",
    "| Lemmatization    | spaCy        | Real base forms (dictionary words) |\n",
    "| POS Tagging      | spaCy        | Tags like NOUN, VERB, etc.         |\n",
    "| NER              | spaCy        | Extract entities like PERSON, DATE |\n",
    "| Vectorization    | scikit-learn | Numerical matrix for modeling      |\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like the same pipeline with `TextBlob`, `transformers`, or deep learning!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4aba27",
   "metadata": {},
   "source": [
    "Sure! Let’s break down **stemming** in **simple, everyday terms**, with clear examples.\n",
    "\n",
    "---\n",
    "\n",
    "## 🌱 What is Stemming?\n",
    "\n",
    "**Stemming** is like chopping off the ends of words to get to the **root form**, even if the result isn’t a real word.\n",
    "\n",
    "### 🔍 Think of it like this:\n",
    "\n",
    "Imagine you're trying to group similar words together. Stemming helps by removing prefixes or suffixes like:\n",
    "\n",
    "* `-ing`, `-ed`, `-ly`, `-es`, `-s`, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠 How it Works (In Simple Terms)\n",
    "\n",
    "You don't care about **perfect grammar** — you just want to match **similar words**.\n",
    "\n",
    "For example:\n",
    "\n",
    "* **“play”, “playing”, “played”, “plays”** → all become → **“play”**\n",
    "* **“studies”, “studied”, “studying”** → → **“studi”** (note: not a real word)\n",
    "\n",
    "### ✅ Why it's useful:\n",
    "\n",
    "It helps search engines or NLP tools recognize that **“playing” and “played” are about the same concept**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Real-Life Example\n",
    "\n",
    "### Sentence:\n",
    "\n",
    "> \"She was playing and studied hard every night.\"\n",
    "\n",
    "### After Stemming:\n",
    "\n",
    "> \"She wa **play** and **studi** hard everi night.\"\n",
    "\n",
    "You can see:\n",
    "\n",
    "* \"playing\" → \"play\"\n",
    "* \"studied\" → \"studi\"\n",
    "* \"every\" → \"everi\"\n",
    "\n",
    "Some of the stemmed words (like **studi** and **everi**) aren’t real English words, but that’s okay — the goal is to match **similar** words quickly.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙ Common Stemming Algorithms:\n",
    "\n",
    "* **Porter Stemmer** (most common, fast, simple)\n",
    "* **Snowball Stemmer** (improved version of Porter)\n",
    "* **Lancaster Stemmer** (more aggressive)\n",
    "\n",
    "---\n",
    "\n",
    "## 🔑 Summary in Layman's Terms:\n",
    "\n",
    "| Feature         | Description                                          |\n",
    "| --------------- | ---------------------------------------------------- |\n",
    "| 📌 What it does | Cuts words to a basic form (may not be a real word)  |\n",
    "| 🧠 Why it helps | Groups similar words together for analysis or search |\n",
    "| ⚠️ Downsides    | Can chop too much or give meaningless words          |\n",
    "| 📚 Example      | “Studies” → “Studi”, “Running” → “Run”               |\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want to compare this with **lemmatization** side by side again — or if you'd like a demo with code!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "22f320bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"\n",
    "The Great question! spaCy and NLTK are two of the most widely used Natural Language Processing (NLP) libraries in Python, but they have different focuses and strengths.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "514c0975",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\chand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cdf5dc32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nThe Great question!',\n",
       " 'spaCy and NLTK are two of the most widely used Natural Language Processing (NLP) libraries in Python, but they have different focuses and strengths.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4e0d62",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "10ec1369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "85f75e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3ca4bcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"Eating\",\"Eaten\",\"Eates\",\"Programming\",\"Programs\",\"Programmer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "11383e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eating ---> eat\n",
      "Eaten ---> eaten\n",
      "Eates ---> eat\n",
      "Programming ---> program\n",
      "Programs ---> program\n",
      "Programmer ---> programm\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(f\"{word} ---> {stemming.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3942d4",
   "metadata": {},
   "source": [
    "### SNOBALL STEMMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cbe06f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "67d86a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "ball_stem = SnowballStemmer(\n",
    "    language='english'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "85f02573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eating-->eat\n",
      "Eaten-->eaten\n",
      "Eates-->eat\n",
      "Programming-->program\n",
      "Programs-->program\n",
      "Programmer-->programm\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(f\"{word}-->{ball_stem.stem(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d394bb07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fairli'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem(\"Fairly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2c7fb06c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fair'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ball_stem.stem(\"Fairly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913e9028",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 What is Lemmatization?\n",
    "\n",
    "**Lemmatization** is the process of reducing a word to its **base or dictionary form** (called a **lemma**). Unlike stemming, it always returns real English words.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 What is WordNetLemmatizer?\n",
    "\n",
    "* `WordNetLemmatizer` is a tool in **NLTK (Natural Language Toolkit)**.\n",
    "* It uses the **WordNet** lexical database to find the **correct lemma** based on the **part of speech (POS)**.\n",
    "* It’s **smarter than stemming** because it considers the context (like verb vs noun).\n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 Difference: Lemmatization vs Stemming\n",
    "\n",
    "| Word      | Stemming | Lemmatization |\n",
    "| --------- | -------- | ------------- |\n",
    "| `studies` | `studi`  | `study`       |\n",
    "| `better`  | `better` | `good`        |\n",
    "| `running` | `run`    | `run`         |\n",
    "| `was`     | `wa`     | `be`          |\n",
    "\n",
    "🔎 **Lemmatization uses grammar rules + dictionary**\n",
    "✂️ **Stemming just chops word endings**\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 Real-World Example (Layman Terms)\n",
    "\n",
    "Imagine a **search engine**. If someone searches for:\n",
    "\n",
    "> “How to **studied** English?”\n",
    "\n",
    "You’d also want it to find:\n",
    "\n",
    "* “study English”\n",
    "* “studying English”\n",
    "* “studies in English”\n",
    "\n",
    "With **lemmatization**, all forms become → `study` ✅\n",
    "This ensures **better search results, accurate text mining, and cleaner data**.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Python Example using WordNetLemmatizer\n",
    "\n",
    "### 🔹 Step-by-step:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')         # For tokenization\n",
    "nltk.download('averaged_perceptron_tagger')  # For POS tagging\n",
    "\n",
    "# Create the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Example text\n",
    "text = \"The children are running and studies were being conducted.\"\n",
    "\n",
    "# Tokenize the text\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Helper to convert NLTK POS tags to WordNet POS tags\n",
    "from nltk.corpus.reader.wordnet import VERB, NOUN, ADJ, ADV\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return ADV\n",
    "    else:\n",
    "        return NOUN  # default\n",
    "\n",
    "# POS tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Lemmatize each word with POS\n",
    "lemmatized = [\n",
    "    lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "    for word, pos in pos_tags\n",
    "]\n",
    "\n",
    "print(\"Original:\", tokens)\n",
    "print(\"Lemmatized:\", lemmatized)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔎 Output:\n",
    "\n",
    "```\n",
    "Original: ['The', 'children', 'are', 'running', 'and', 'studies', 'were', 'being', 'conducted', '.']\n",
    "Lemmatized: ['The', 'child', 'be', 'run', 'and', 'study', 'be', 'be', 'conduct', '.']\n",
    "```\n",
    "\n",
    "💡 Notice:\n",
    "\n",
    "* \"children\" → \"child\"\n",
    "* \"running\" → \"run\"\n",
    "* \"studies\" → \"study\"\n",
    "* \"conducted\" → \"conduct\"\n",
    "* \"being\" → \"be\"\n",
    "\n",
    "✅ These are the **true base forms** used in dictionaries.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 When to Use Lemmatization?\n",
    "\n",
    "Use **lemmatization** when:\n",
    "\n",
    "* You care about **grammatically correct base forms**\n",
    "* You're building search engines, chatbots, question answering systems\n",
    "* You need **clean input** for sentiment analysis, classification, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Summary\n",
    "\n",
    "| Concept                   | Description                                 |\n",
    "| ------------------------- | ------------------------------------------- |\n",
    "| **Lemmatization**         | Converts a word to its dictionary form      |\n",
    "| **WordNetLemmatizer**     | NLTK tool that uses WordNet dictionary      |\n",
    "| **Needs POS?**            | Yes — works better with part-of-speech tags |\n",
    "| **Better than stemming?** | Yes, in most real-world NLP tasks           |\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0924ea2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\chand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "de6c78df",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2e5645fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eating\n",
      "Eaten\n",
      "Eates\n",
      "Programming\n",
      "Programs\n",
      "Programmer\n"
     ]
    }
   ],
   "source": [
    "for i in words:\n",
    "    print(lemma.lemmatize(i,pos = 'n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6176cc9f",
   "metadata": {},
   "source": [
    "## Lemma Usecase\n",
    "### Q&A Chatbot text summarizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2f7161",
   "metadata": {},
   "source": [
    "## STOP WORDDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f486322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "para = \"\"\"\n",
    "Natural Language Processing (NLP) is a subfield of artificial intelligence. It focuses on enabling computers to understand and process human languages. Many real-world applications such as chatbots, sentiment analysis, and machine translation use NLP.\n",
    "\n",
    "However, processing raw text isn't straightforward. It requires multiple steps like tokenization, stopword removal, stemming or lemmatization, and vectorization. Each step helps simplify and clean the data before using it in machine learning models.\n",
    "\n",
    "Removing stopwords like \"is\", \"the\", \"and\", or \"in\" helps reduce noise and improves model accuracy. These words are common but usually don't add much meaning to the content.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "43513831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Natural Language Processing (NLP) is a subfield of artificial intelligence. It focuses on enabling computers to understand and process human languages. Many real-world applications such as chatbots, sentiment analysis, and machine translation use NLP.\n",
      "\n",
      "However, processing raw text isn't straightforward. It requires multiple steps like tokenization, stopword removal, stemming or lemmatization, and vectorization. Each step helps simplify and clean the data before using it in machine learning models.\n",
      "\n",
      "Removing stopwords like \"is\", \"the\", \"and\", or \"in\" helps reduce noise and improves model accuracy. These words are common but usually don't add much meaning to the content.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e27c17d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2973f06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "96c84792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4569a3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "67d9ff76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0d60680e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e2cf4347",
   "metadata": {},
   "outputs": [],
   "source": [
    "para_toekn = nltk.sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "27297b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\chand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Download resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "10f625ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Sentences:\n",
      " {0: 'natur languag process nlp subfield artifici intellig natur', 1: 'focus enabl comput understand process human languag', 2: 'mani applic comput chatbot sentiment analysi machin translat use nlp'}\n",
      "\n",
      "Lemmatized Sentences:\n",
      " {0: 'natur languag process nlp subfield artifici intellig natur', 1: 'focus enabl comput understand process human languag', 2: 'mani applic comput chatbot sentiment analysi machin translat use nlp'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Sample paragraph\n",
    "para = \"\"\"\n",
    "Natural Language Processing (NLP) is a subfield of artificial intelligence Naturals. It focuses on enabling computers to understand and process human languages. Many real-world applications computer such as chatbots, sentiment analysis, and machine translation use NLP.\n",
    "\"\"\"\n",
    "\n",
    "# Setup tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Tokenize paragraph into sentences\n",
    "para_token = sent_tokenize(para)\n",
    "\n",
    "# Dictionaries to hold processed sentences\n",
    "sentence_stem = {}\n",
    "sentence_lemma = {}\n",
    "\n",
    "for i in range(len(para_token)):\n",
    "    # Tokenize each sentence into words\n",
    "    words = word_tokenize(para_token[i])\n",
    "\n",
    "    # Remove stopwords and punctuation\n",
    "    filtered = [word.lower() for word in words if word.lower() not in stop_words and word.isalpha()]\n",
    "\n",
    "    # Apply stemming\n",
    "    stem_words = [stemmer.stem(word) for word in filtered]\n",
    "\n",
    "    # Apply lemmatization (default POS='n' for noun)\n",
    "    lemma_words = [lemmatizer.lemmatize(word,pos='n') for word in stem_words]\n",
    "\n",
    "    # Save results\n",
    "    sentence_stem[i] = ' '.join(stem_words)\n",
    "    sentence_lemma[i] = ' '.join(lemma_words)\n",
    "\n",
    "# Output\n",
    "print(\"Stemmed Sentences:\\n\", sentence_stem)\n",
    "print(\"\\nLemmatized Sentences:\\n\", sentence_lemma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d00534cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'natur languag process nlp subfield artifici intellig natur',\n",
       " 1: 'focus enabl comput understand process human languag',\n",
       " 2: 'mani applic comput chatbot sentiment analysi machin translat use nlp'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c7237e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'natur languag process nlp subfield artifici intellig natur',\n",
       " 1: 'focus enabl comput understand process human languag',\n",
       " 2: 'mani applic comput chatbot sentiment analysi machin translat use nlp'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "33d06865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'natur languag process nlp subfield artifici intellig natur',\n",
       " 1: 'focus enabl comput understand process human languag',\n",
       " 2: 'mani applic comput chatbot sentiment analysi machin translat use nlp'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8366fb39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'natur languag process nlp subfield artifici intellig natur',\n",
       " 1: 'focus enabl comput understand process human languag',\n",
       " 2: 'mani applic comput chatbot sentiment analysi machin translat use nlp'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1af35c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"dogs\",pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2d9caa49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\chand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "64a19740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John', 'NNP')]\n",
      "[('is', 'VBZ')]\n",
      "[('good', 'JJ')]\n",
      "[('man', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "for i in \"John is good man\".split():\n",
    "       print(nltk.pos_tag([i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333d3e4a",
   "metadata": {},
   "source": [
    "[Here is link of details about the pos tag meaning](https://www.geeksforgeeks.org/python/part-speech-tagging-stop-words-using-nltk-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f0fc6c",
   "metadata": {},
   "source": [
    "### NAMED ENTITY REGOGNIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "175c54b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(para)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4e5c87ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " '(',\n",
       " 'NLP',\n",
       " ')',\n",
       " 'is',\n",
       " 'a',\n",
       " 'subfield',\n",
       " 'of',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'Naturals',\n",
       " '.',\n",
       " 'It',\n",
       " 'focuses',\n",
       " 'on',\n",
       " 'enabling',\n",
       " 'computers',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'and',\n",
       " 'process',\n",
       " 'human',\n",
       " 'languages',\n",
       " '.',\n",
       " 'Many',\n",
       " 'real-world',\n",
       " 'applications',\n",
       " 'computer',\n",
       " 'such',\n",
       " 'as',\n",
       " 'chatbots',\n",
       " ',',\n",
       " 'sentiment',\n",
       " 'analysis',\n",
       " ',',\n",
       " 'and',\n",
       " 'machine',\n",
       " 'translation',\n",
       " 'use',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2a6aad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_element = nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "609ded55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\chand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\chand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9cb60c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "value  = nltk.ne_chunk(tag_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "886d32e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Natural/JJ\n",
      "  Language/NNP\n",
      "  Processing/NNP\n",
      "  (/(\n",
      "  (ORGANIZATION NLP/NNP)\n",
      "  )/)\n",
      "  is/VBZ\n",
      "  a/DT\n",
      "  subfield/NN\n",
      "  of/IN\n",
      "  artificial/JJ\n",
      "  intelligence/NN\n",
      "  Naturals/NNS\n",
      "  ./.\n",
      "  It/PRP\n",
      "  focuses/VBZ\n",
      "  on/IN\n",
      "  enabling/VBG\n",
      "  computers/NNS\n",
      "  to/TO\n",
      "  understand/VB\n",
      "  and/CC\n",
      "  process/VB\n",
      "  human/JJ\n",
      "  languages/NNS\n",
      "  ./.\n",
      "  Many/JJ\n",
      "  real-world/JJ\n",
      "  applications/NNS\n",
      "  computer/NN\n",
      "  such/JJ\n",
      "  as/IN\n",
      "  chatbots/NNS\n",
      "  ,/,\n",
      "  sentiment/NN\n",
      "  analysis/NN\n",
      "  ,/,\n",
      "  and/CC\n",
      "  machine/NN\n",
      "  translation/NN\n",
      "  use/NN\n",
      "  (ORGANIZATION NLP/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d1890b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,2728.0,168.0\" width=\"2728px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"2.6393%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Natural</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"1.31965%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.93255%\" x=\"2.6393%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Language</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"4.10557%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.51906%\" x=\"5.57185%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Processing</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"7.33138%\" y1=\"20px\" y2=\"48px\" /><svg width=\"0.879765%\" x=\"9.09091%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">(</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">(</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"9.53079%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.10557%\" x=\"9.97067%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ORGANIZATION</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NLP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"12.0235%\" y1=\"20px\" y2=\"48px\" /><svg width=\"0.879765%\" x=\"14.0762%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">)</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">)</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"14.5161%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.46628%\" x=\"14.956%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">is</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBZ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"15.6891%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.17302%\" x=\"16.4223%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">a</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"17.0088%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.93255%\" x=\"17.5953%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">subfield</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"19.0616%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.17302%\" x=\"20.5279%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">of</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"21.1144%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.51906%\" x=\"21.7009%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">artificial</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"23.4604%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.10557%\" x=\"25.2199%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">intelligence</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"27.2727%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.93255%\" x=\"29.3255%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Naturals</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"30.7918%\" y1=\"20px\" y2=\"48px\" /><svg width=\"0.879765%\" x=\"32.2581%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"32.6979%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.46628%\" x=\"33.1378%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">It</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PRP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"33.871%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.6393%\" x=\"34.6041%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">focuses</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBZ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"35.9238%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.17302%\" x=\"37.2434%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">on</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"37.8299%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.93255%\" x=\"38.4164%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">enabling</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBG</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"39.8827%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.22581%\" x=\"41.349%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">computers</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"42.9619%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.17302%\" x=\"44.5748%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">to</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">TO</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"45.1613%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.51906%\" x=\"45.7478%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">understand</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"47.5073%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.46628%\" x=\"49.2669%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">and</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CC</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.6393%\" x=\"50.7331%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">process</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"52.0528%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.05279%\" x=\"53.3724%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">human</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"54.3988%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.22581%\" x=\"55.4252%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">languages</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"57.0381%\" y1=\"20px\" y2=\"48px\" /><svg width=\"0.879765%\" x=\"58.651%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"59.0909%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.75953%\" x=\"59.5308%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Many</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"60.4106%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.51906%\" x=\"61.2903%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">real-world</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"63.0499%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.10557%\" x=\"64.8094%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">applications</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"66.8622%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.93255%\" x=\"68.915%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">computer</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"70.3812%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.75953%\" x=\"71.8475%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">such</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"72.7273%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.17302%\" x=\"73.607%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">as</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"74.1935%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.93255%\" x=\"74.7801%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">chatbots</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.2463%\" y1=\"20px\" y2=\"48px\" /><svg width=\"0.879765%\" x=\"77.7126%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"78.1525%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.22581%\" x=\"78.5924%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">sentiment</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"80.2053%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.93255%\" x=\"81.8182%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">analysis</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"83.2845%\" y1=\"20px\" y2=\"48px\" /><svg width=\"0.879765%\" x=\"84.7507%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"85.1906%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.46628%\" x=\"85.6305%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">and</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CC</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"86.3636%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.6393%\" x=\"87.0968%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">machine</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"88.4164%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.81232%\" x=\"89.7361%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">translation</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"91.6422%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.46628%\" x=\"93.5484%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">use</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"94.2815%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.10557%\" x=\"95.0147%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ORGANIZATION</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NLP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"97.0674%\" y1=\"20px\" y2=\"48px\" /><svg width=\"0.879765%\" x=\"99.1202%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"99.5601%\" y1=\"20px\" y2=\"48px\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), Tree('ORGANIZATION', [('NLP', 'NNP')]), (')', ')'), ('is', 'VBZ'), ('a', 'DT'), ('subfield', 'NN'), ('of', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('Naturals', 'NNS'), ('.', '.'), ('It', 'PRP'), ('focuses', 'VBZ'), ('on', 'IN'), ('enabling', 'VBG'), ('computers', 'NNS'), ('to', 'TO'), ('understand', 'VB'), ('and', 'CC'), ('process', 'VB'), ('human', 'JJ'), ('languages', 'NNS'), ('.', '.'), ('Many', 'JJ'), ('real-world', 'JJ'), ('applications', 'NNS'), ('computer', 'NN'), ('such', 'JJ'), ('as', 'IN'), ('chatbots', 'NNS'), (',', ','), ('sentiment', 'NN'), ('analysis', 'NN'), (',', ','), ('and', 'CC'), ('machine', 'NN'), ('translation', 'NN'), ('use', 'NN'), Tree('ORGANIZATION', [('NLP', 'NNP')]), ('.', '.')])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f249fb",
   "metadata": {},
   "source": [
    "# SPACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5d9f129e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.24.4\n",
      "  Downloading numpy-1.24.4.tar.gz (10.9 MB)\n",
      "     ---------------------------------------- 0.0/10.9 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 1.0/10.9 MB 10.2 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 4.2/10.9 MB 12.8 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 8.1/10.9 MB 15.3 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 9.7/10.9 MB 13.3 MB/s eta 0:00:01\n",
      "     --------------------------------------  10.7/10.9 MB 12.2 MB/s eta 0:00:01\n",
      "     --------------------------------------- 10.9/10.9 MB 11.2 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [32 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \u001b[35m\"E:\\PROJECTS\\TEST\\Learning\\Placement-Preperations\\venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m389\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "          \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "          \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "        File \u001b[35m\"E:\\PROJECTS\\TEST\\Learning\\Placement-Preperations\\venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m373\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "          json_out[\"return_val\"] = \u001b[31mhook\u001b[0m\u001b[1;31m(**hook_input[\"kwargs\"])\u001b[0m\n",
      "                                   \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"E:\\PROJECTS\\TEST\\Learning\\Placement-Preperations\\venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m137\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "          backend = _build_backend()\n",
      "        File \u001b[35m\"E:\\PROJECTS\\TEST\\Learning\\Placement-Preperations\\venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m70\u001b[0m, in \u001b[35m_build_backend\u001b[0m\n",
      "          obj = import_module(mod_path)\n",
      "        File \u001b[35m\"C:\\Users\\chand\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\importlib\\__init__.py\"\u001b[0m, line \u001b[35m88\u001b[0m, in \u001b[35mimport_module\u001b[0m\n",
      "          return \u001b[31m_bootstrap._gcd_import\u001b[0m\u001b[1;31m(name[level:], package, level)\u001b[0m\n",
      "                 \u001b[31m~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[35m1387\u001b[0m, in \u001b[35m_gcd_import\u001b[0m\n",
      "        File \u001b[35m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[35m1360\u001b[0m, in \u001b[35m_find_and_load\u001b[0m\n",
      "        File \u001b[35m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[35m1310\u001b[0m, in \u001b[35m_find_and_load_unlocked\u001b[0m\n",
      "        File \u001b[35m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[35m488\u001b[0m, in \u001b[35m_call_with_frames_removed\u001b[0m\n",
      "        File \u001b[35m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[35m1387\u001b[0m, in \u001b[35m_gcd_import\u001b[0m\n",
      "        File \u001b[35m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[35m1360\u001b[0m, in \u001b[35m_find_and_load\u001b[0m\n",
      "        File \u001b[35m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[35m1331\u001b[0m, in \u001b[35m_find_and_load_unlocked\u001b[0m\n",
      "        File \u001b[35m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[35m935\u001b[0m, in \u001b[35m_load_unlocked\u001b[0m\n",
      "        File \u001b[35m\"<frozen importlib._bootstrap_external>\"\u001b[0m, line \u001b[35m1026\u001b[0m, in \u001b[35mexec_module\u001b[0m\n",
      "        File \u001b[35m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[35m488\u001b[0m, in \u001b[35m_call_with_frames_removed\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\chand\\AppData\\Local\\Temp\\pip-build-env-g88awmfl\\overlay\\Lib\\site-packages\\setuptools\\__init__.py\"\u001b[0m, line \u001b[35m16\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "          import setuptools.version\n",
      "        File \u001b[35m\"C:\\Users\\chand\\AppData\\Local\\Temp\\pip-build-env-g88awmfl\\overlay\\Lib\\site-packages\\setuptools\\version.py\"\u001b[0m, line \u001b[35m1\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "          import pkg_resources\n",
      "        File \u001b[35m\"C:\\Users\\chand\\AppData\\Local\\Temp\\pip-build-env-g88awmfl\\overlay\\Lib\\site-packages\\pkg_resources\\__init__.py\"\u001b[0m, line \u001b[35m2172\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "          register_finder(\u001b[1;31mpkgutil.ImpImporter\u001b[0m, find_on_path)\n",
      "                          \u001b[1;31m^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "      \u001b[1;35mAttributeError\u001b[0m: \u001b[35mmodule 'pkgutil' has no attribute 'ImpImporter'. Did you mean: 'zipimporter'?\u001b[0m\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× Getting requirements to build wheel did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.24.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "28e61b64",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing numpy_ops: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[84]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacy\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Load spaCy English model\u001b[39;00m\n\u001b[32m      4\u001b[39m nlp = spacy.load(\u001b[33m\"\u001b[39m\u001b[33men_core_web_sm\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\PROJECTS\\TEST\\Learning\\Placement-Preperations\\venv\\Lib\\site-packages\\spacy\\__init__.py:6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Iterable, Union\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# set library-specific custom warning handling before doing anything else\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m setup_default_warnings\n\u001b[32m      8\u001b[39m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\PROJECTS\\TEST\\Learning\\Placement-Preperations\\venv\\Lib\\site-packages\\spacy\\errors.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mErrorsWithCodes\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m):\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, code):\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\PROJECTS\\TEST\\Learning\\Placement-Preperations\\venv\\Lib\\site-packages\\spacy\\compat.py:39\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcatalogue\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _importlib_metadata \u001b[38;5;28;01mas\u001b[39;00m importlib_metadata  \u001b[38;5;66;03m# type: ignore[no-redef]    # noqa: F401\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mthinc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optimizer  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     41\u001b[39m pickle = pickle\n\u001b[32m     42\u001b[39m copy_reg = copy_reg\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\PROJECTS\\TEST\\Learning\\Placement-Preperations\\venv\\Lib\\site-packages\\thinc\\api.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     CupyOps,\n\u001b[32m      3\u001b[39m     MPSOps,\n\u001b[32m      4\u001b[39m     NumpyOps,\n\u001b[32m      5\u001b[39m     Ops,\n\u001b[32m      6\u001b[39m     get_current_ops,\n\u001b[32m      7\u001b[39m     get_ops,\n\u001b[32m      8\u001b[39m     set_current_ops,\n\u001b[32m      9\u001b[39m     set_gpu_allocator,\n\u001b[32m     10\u001b[39m     use_ops,\n\u001b[32m     11\u001b[39m     use_pytorch_for_gpu_memory,\n\u001b[32m     12\u001b[39m     use_tensorflow_for_gpu_memory,\n\u001b[32m     13\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m enable_mxnet, enable_tensorflow, has_cupy\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Config, ConfigValidationError, registry\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\PROJECTS\\TEST\\Learning\\Placement-Preperations\\venv\\Lib\\site-packages\\thinc\\backends\\__init__.py:17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_cupy_allocators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cupy_pytorch_allocator, cupy_tensorflow_allocator\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_server\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParamServer\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcupy_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CupyOps\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmps_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MPSOps\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumpy_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NumpyOps\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\PROJECTS\\TEST\\Learning\\Placement-Preperations\\venv\\Lib\\site-packages\\thinc\\backends\\cupy_ops.py:16\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      7\u001b[39m     is_cupy_array,\n\u001b[32m      8\u001b[39m     is_mxnet_gpu_array,\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m     torch2xp,\n\u001b[32m     14\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _custom_kernels\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumpy_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NumpyOps\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Ops\n\u001b[32m     20\u001b[39m \u001b[38;5;129m@registry\u001b[39m.ops(\u001b[33m\"\u001b[39m\u001b[33mCupyOps\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mCupyOps\u001b[39;00m(Ops):\n",
      "\u001b[31mImportError\u001b[39m: DLL load failed while importing numpy_ops: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"Elon Musk founded SpaceX and Tesla. He was born in South Africa and lives in the United States.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract and display named entities\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:<25} --> {ent.label_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6305bbf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
