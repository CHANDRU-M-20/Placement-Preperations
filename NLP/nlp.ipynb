{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05f8311c",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc1ddc3",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 🔄 NLP Pipeline Overview\n",
    "\n",
    "1. **Text Input**\n",
    "2. **Text Cleaning / Preprocessing**\n",
    "3. **Tokenization**\n",
    "4. **Stopword Removal**\n",
    "5. **Stemming / Lemmatization**\n",
    "6. **Part-of-Speech Tagging**\n",
    "7. **Named Entity Recognition**\n",
    "8. **Vectorization** (for ML)\n",
    "9. **Modeling / Analysis**\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Step-by-Step Pipeline with Code\n",
    "\n",
    "We’ll use **NLTK** for stemming and **spaCy** for lemmatization and advanced tasks.\n",
    "\n",
    "### 🔹 1. Text Input\n",
    "\n",
    "```python\n",
    "text = \"John's running in the marathon was better than his previous attempts. He studies NLP techniques daily.\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 2. Text Cleaning\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "# Remove punctuation and lowercase\n",
    "clean_text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "print(clean_text)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "`johns running in the marathon was better than his previous attempts he studies nlp techniques daily`\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 3. Tokenization\n",
    "\n",
    "```python\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "tokens = word_tokenize(clean_text)\n",
    "print(tokens)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "`['johns', 'running', 'in', 'the', 'marathon', 'was', 'better', 'than', 'his', 'previous', 'attempts', 'he', 'studies', 'nlp', 'techniques', 'daily']`\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 4. Stopword Removal\n",
    "\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "print(filtered_tokens)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "`['johns', 'running', 'marathon', 'better', 'previous', 'attempts', 'studies', 'nlp', 'techniques', 'daily']`\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 5. Stemming (using NLTK)\n",
    "\n",
    "```python\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(stemmed)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "`['john', 'run', 'marathon', 'better', 'previou', 'attempt', 'studi', 'nlp', 'techniqu', 'daili']`\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 6. Lemmatization (using spaCy)\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\" \".join(filtered_tokens))\n",
    "lemmatized = [token.lemma_ for token in doc]\n",
    "print(lemmatized)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "`['john', 'run', 'marathon', 'well', 'previous', 'attempt', 'study', 'NLP', 'technique', 'daily']`\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 7. POS Tagging\n",
    "\n",
    "```python\n",
    "for token in doc:\n",
    "    print((token.text, token.pos_))\n",
    "```\n",
    "\n",
    "**Example Output**:\n",
    "\n",
    "```\n",
    "('john', 'PROPN')\n",
    "('run', 'VERB')\n",
    "('marathon', 'NOUN')\n",
    "('well', 'ADV')\n",
    "...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 8. Named Entity Recognition (NER)\n",
    "\n",
    "```python\n",
    "doc = nlp(text)  # use original text\n",
    "for ent in doc.ents:\n",
    "    print((ent.text, ent.label_))\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "\n",
    "```\n",
    "('John', 'PERSON')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 9. Vectorization (Optional for ML)\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\"John is running a marathon.\", \"He studies NLP.\"]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Summary\n",
    "\n",
    "| Stage            | Tool Used    | Output                             |\n",
    "| ---------------- | ------------ | ---------------------------------- |\n",
    "| Cleaning         | Regex        | Clean, lowercase text              |\n",
    "| Tokenization     | NLTK / spaCy | List of words                      |\n",
    "| Stopword Removal | NLTK         | Filtered tokens                    |\n",
    "| Stemming         | NLTK         | Root-like forms (may be non-words) |\n",
    "| Lemmatization    | spaCy        | Real base forms (dictionary words) |\n",
    "| POS Tagging      | spaCy        | Tags like NOUN, VERB, etc.         |\n",
    "| NER              | spaCy        | Extract entities like PERSON, DATE |\n",
    "| Vectorization    | scikit-learn | Numerical matrix for modeling      |\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like the same pipeline with `TextBlob`, `transformers`, or deep learning!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4aba27",
   "metadata": {},
   "source": [
    "Sure! Let’s break down **stemming** in **simple, everyday terms**, with clear examples.\n",
    "\n",
    "---\n",
    "\n",
    "## 🌱 What is Stemming?\n",
    "\n",
    "**Stemming** is like chopping off the ends of words to get to the **root form**, even if the result isn’t a real word.\n",
    "\n",
    "### 🔍 Think of it like this:\n",
    "\n",
    "Imagine you're trying to group similar words together. Stemming helps by removing prefixes or suffixes like:\n",
    "\n",
    "* `-ing`, `-ed`, `-ly`, `-es`, `-s`, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠 How it Works (In Simple Terms)\n",
    "\n",
    "You don't care about **perfect grammar** — you just want to match **similar words**.\n",
    "\n",
    "For example:\n",
    "\n",
    "* **“play”, “playing”, “played”, “plays”** → all become → **“play”**\n",
    "* **“studies”, “studied”, “studying”** → → **“studi”** (note: not a real word)\n",
    "\n",
    "### ✅ Why it's useful:\n",
    "\n",
    "It helps search engines or NLP tools recognize that **“playing” and “played” are about the same concept**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Real-Life Example\n",
    "\n",
    "### Sentence:\n",
    "\n",
    "> \"She was playing and studied hard every night.\"\n",
    "\n",
    "### After Stemming:\n",
    "\n",
    "> \"She wa **play** and **studi** hard everi night.\"\n",
    "\n",
    "You can see:\n",
    "\n",
    "* \"playing\" → \"play\"\n",
    "* \"studied\" → \"studi\"\n",
    "* \"every\" → \"everi\"\n",
    "\n",
    "Some of the stemmed words (like **studi** and **everi**) aren’t real English words, but that’s okay — the goal is to match **similar** words quickly.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙ Common Stemming Algorithms:\n",
    "\n",
    "* **Porter Stemmer** (most common, fast, simple)\n",
    "* **Snowball Stemmer** (improved version of Porter)\n",
    "* **Lancaster Stemmer** (more aggressive)\n",
    "\n",
    "---\n",
    "\n",
    "## 🔑 Summary in Layman's Terms:\n",
    "\n",
    "| Feature         | Description                                          |\n",
    "| --------------- | ---------------------------------------------------- |\n",
    "| 📌 What it does | Cuts words to a basic form (may not be a real word)  |\n",
    "| 🧠 Why it helps | Groups similar words together for analysis or search |\n",
    "| ⚠️ Downsides    | Can chop too much or give meaningless words          |\n",
    "| 📚 Example      | “Studies” → “Studi”, “Running” → “Run”               |\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want to compare this with **lemmatization** side by side again — or if you'd like a demo with code!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22f320bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"\n",
    "The Great question! spaCy and NLTK are two of the most widely used Natural Language Processing (NLP) libraries in Python, but they have different focuses and strengths.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "514c0975",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\chand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdf5dc32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nThe Great question!',\n",
       " 'spaCy and NLTK are two of the most widely used Natural Language Processing (NLP) libraries in Python, but they have different focuses and strengths.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4e0d62",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10ec1369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85f75e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ca4bcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"Eating\",\"Eaten\",\"Eates\",\"Programming\",\"Programs\",\"Programmer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11383e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eating ---> eat\n",
      "Eaten ---> eaten\n",
      "Eates ---> eat\n",
      "Programming ---> program\n",
      "Programs ---> program\n",
      "Programmer ---> programm\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(f\"{word} ---> {stemming.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3942d4",
   "metadata": {},
   "source": [
    "### SNOBALL STEMMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbe06f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67d86a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "ball_stem = SnowballStemmer(\n",
    "    language='english'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85f02573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eating-->eat\n",
      "Eaten-->eaten\n",
      "Eates-->eat\n",
      "Programming-->program\n",
      "Programs-->program\n",
      "Programmer-->programm\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(f\"{word}-->{ball_stem.stem(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d394bb07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fairli'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem(\"Fairly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c7fb06c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fair'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ball_stem.stem(\"Fairly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913e9028",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 What is Lemmatization?\n",
    "\n",
    "**Lemmatization** is the process of reducing a word to its **base or dictionary form** (called a **lemma**). Unlike stemming, it always returns real English words.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 What is WordNetLemmatizer?\n",
    "\n",
    "* `WordNetLemmatizer` is a tool in **NLTK (Natural Language Toolkit)**.\n",
    "* It uses the **WordNet** lexical database to find the **correct lemma** based on the **part of speech (POS)**.\n",
    "* It’s **smarter than stemming** because it considers the context (like verb vs noun).\n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 Difference: Lemmatization vs Stemming\n",
    "\n",
    "| Word      | Stemming | Lemmatization |\n",
    "| --------- | -------- | ------------- |\n",
    "| `studies` | `studi`  | `study`       |\n",
    "| `better`  | `better` | `good`        |\n",
    "| `running` | `run`    | `run`         |\n",
    "| `was`     | `wa`     | `be`          |\n",
    "\n",
    "🔎 **Lemmatization uses grammar rules + dictionary**\n",
    "✂️ **Stemming just chops word endings**\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 Real-World Example (Layman Terms)\n",
    "\n",
    "Imagine a **search engine**. If someone searches for:\n",
    "\n",
    "> “How to **studied** English?”\n",
    "\n",
    "You’d also want it to find:\n",
    "\n",
    "* “study English”\n",
    "* “studying English”\n",
    "* “studies in English”\n",
    "\n",
    "With **lemmatization**, all forms become → `study` ✅\n",
    "This ensures **better search results, accurate text mining, and cleaner data**.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Python Example using WordNetLemmatizer\n",
    "\n",
    "### 🔹 Step-by-step:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')         # For tokenization\n",
    "nltk.download('averaged_perceptron_tagger')  # For POS tagging\n",
    "\n",
    "# Create the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Example text\n",
    "text = \"The children are running and studies were being conducted.\"\n",
    "\n",
    "# Tokenize the text\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Helper to convert NLTK POS tags to WordNet POS tags\n",
    "from nltk.corpus.reader.wordnet import VERB, NOUN, ADJ, ADV\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return ADV\n",
    "    else:\n",
    "        return NOUN  # default\n",
    "\n",
    "# POS tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Lemmatize each word with POS\n",
    "lemmatized = [\n",
    "    lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "    for word, pos in pos_tags\n",
    "]\n",
    "\n",
    "print(\"Original:\", tokens)\n",
    "print(\"Lemmatized:\", lemmatized)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔎 Output:\n",
    "\n",
    "```\n",
    "Original: ['The', 'children', 'are', 'running', 'and', 'studies', 'were', 'being', 'conducted', '.']\n",
    "Lemmatized: ['The', 'child', 'be', 'run', 'and', 'study', 'be', 'be', 'conduct', '.']\n",
    "```\n",
    "\n",
    "💡 Notice:\n",
    "\n",
    "* \"children\" → \"child\"\n",
    "* \"running\" → \"run\"\n",
    "* \"studies\" → \"study\"\n",
    "* \"conducted\" → \"conduct\"\n",
    "* \"being\" → \"be\"\n",
    "\n",
    "✅ These are the **true base forms** used in dictionaries.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 When to Use Lemmatization?\n",
    "\n",
    "Use **lemmatization** when:\n",
    "\n",
    "* You care about **grammatically correct base forms**\n",
    "* You're building search engines, chatbots, question answering systems\n",
    "* You need **clean input** for sentiment analysis, classification, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Summary\n",
    "\n",
    "| Concept                   | Description                                 |\n",
    "| ------------------------- | ------------------------------------------- |\n",
    "| **Lemmatization**         | Converts a word to its dictionary form      |\n",
    "| **WordNetLemmatizer**     | NLTK tool that uses WordNet dictionary      |\n",
    "| **Needs POS?**            | Yes — works better with part-of-speech tags |\n",
    "| **Better than stemming?** | Yes, in most real-world NLP tasks           |\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0924ea2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\chand\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de6c78df",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e5645fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eating\n",
      "Eaten\n",
      "Eates\n",
      "Programming\n",
      "Programs\n",
      "Programmer\n"
     ]
    }
   ],
   "source": [
    "for i in words:\n",
    "    print(lemma.lemmatize(i,pos = 'n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6176cc9f",
   "metadata": {},
   "source": [
    "## Lemma Usecase\n",
    "### Q&A Chatbot text summarizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2f7161",
   "metadata": {},
   "source": [
    "## STOP WORDDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f486322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "para = \"\"\"\n",
    "Natural Language Processing (NLP) is a subfield of artificial intelligence. It focuses on enabling computers to understand and process human languages. Many real-world applications such as chatbots, sentiment analysis, and machine translation use NLP.\n",
    "\n",
    "However, processing raw text isn't straightforward. It requires multiple steps like tokenization, stopword removal, stemming or lemmatization, and vectorization. Each step helps simplify and clean the data before using it in machine learning models.\n",
    "\n",
    "Removing stopwords like \"is\", \"the\", \"and\", or \"in\" helps reduce noise and improves model accuracy. These words are common but usually don't add much meaning to the content.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "43513831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Natural Language Processing (NLP) is a subfield of artificial intelligence. It focuses on enabling computers to understand and process human languages. Many real-world applications such as chatbots, sentiment analysis, and machine translation use NLP.\n",
      "\n",
      "However, processing raw text isn't straightforward. It requires multiple steps like tokenization, stopword removal, stemming or lemmatization, and vectorization. Each step helps simplify and clean the data before using it in machine learning models.\n",
      "\n",
      "Removing stopwords like \"is\", \"the\", \"and\", or \"in\" helps reduce noise and improves model accuracy. These words are common but usually don't add much meaning to the content.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e27c17d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2973f06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "96c84792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4569a3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "67d9ff76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d60680e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e2cf4347",
   "metadata": {},
   "outputs": [],
   "source": [
    "para_toekn = nltk.sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "10f625ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Sentences:\n",
      " {0: 'natur languag process nlp subfield artifici intellig', 1: 'focus enabl comput understand process human languag', 2: 'mani applic chatbot sentiment analysi machin translat use nlp'}\n",
      "\n",
      "Lemmatized Sentences:\n",
      " {0: 'natur languag process nlp subfield artifici intellig', 1: 'focus enabl comput understand process human languag', 2: 'mani applic chatbot sentiment analysi machin translat use nlp'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\chand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Download resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Sample paragraph\n",
    "para = \"\"\"\n",
    "Natural Language Processing (NLP) is a subfield of artificial intelligence. It focuses on enabling computers to understand and process human languages. Many real-world applications such as chatbots, sentiment analysis, and machine translation use NLP.\n",
    "\"\"\"\n",
    "\n",
    "# Setup tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Tokenize paragraph into sentences\n",
    "para_token = sent_tokenize(para)\n",
    "\n",
    "# Dictionaries to hold processed sentences\n",
    "sentence_stem = {}\n",
    "sentence_lemma = {}\n",
    "\n",
    "for i in range(len(para_token)):\n",
    "    # Tokenize each sentence into words\n",
    "    words = word_tokenize(para_token[i])\n",
    "\n",
    "    # Remove stopwords and punctuation\n",
    "    filtered = [word.lower() for word in words if word.lower() not in stop_words and word.isalpha()]\n",
    "\n",
    "    # Apply stemming\n",
    "    stem_words = [stemmer.stem(word) for word in filtered]\n",
    "\n",
    "    # Apply lemmatization (default POS='n' for noun)\n",
    "    lemma_words = [lemmatizer.lemmatize(word,pos='v') for word in stem_words]\n",
    "\n",
    "    # Save results\n",
    "    sentence_stem[i] = ' '.join(stem_words)\n",
    "    sentence_lemma[i] = ' '.join(lemma_words)\n",
    "\n",
    "# Output\n",
    "print(\"Stemmed Sentences:\\n\", sentence_stem)\n",
    "print(\"\\nLemmatized Sentences:\\n\", sentence_lemma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d00534cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'natur languag process nlp subfield artifici intellig',\n",
       " 1: 'focus enabl comput understand process human languag',\n",
       " 2: 'mani applic chatbot sentiment analysi machin translat use nlp'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c7237e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'natur languag process nlp subfield artifici intellig',\n",
       " 1: 'focus enabl comput understand process human languag',\n",
       " 2: 'mani applic chatbot sentiment analysi machin translat use nlp'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "33d06865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'natur languag process ( nlp ) subfield artifici intellig .',\n",
       " 1: 'it focus enabl comput understand process human languag .',\n",
       " 2: 'mani real-world applic chatbot , sentiment analysi , machin translat use nlp .',\n",
       " 3: \"howev , process raw text n't straightforward .\",\n",
       " 4: 'it requir multipl step like token , stopword remov , stem lemmat , vector .',\n",
       " 5: 'each step help simplifi clean data use machin learn model .',\n",
       " 6: \"remov stopword like `` '' , `` '' , `` '' , `` '' help reduc nois improv model accuraci .\",\n",
       " 7: \"these word common usual n't add much mean content .\"}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8366fb39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'natur languag process ( nlp ) subfield artifici intellig .',\n",
       " 1: 'it focus enabl comput understand process human languag .',\n",
       " 2: 'mani real-world applic chatbot , sentiment analysi , machin translat use nlp .',\n",
       " 3: \"howev , process raw text n't straightforward .\",\n",
       " 4: 'it requir multipl step like token , stopword remov , stem lemmat , vector .',\n",
       " 5: 'each step help simplifi clean data use machin learn model .',\n",
       " 6: \"remov stopword like `` '' , `` '' , `` '' , `` '' help reduc nois improv model accuraci .\",\n",
       " 7: \"these word common usual n't add much mean content .\"}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1af35c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"dogs\",pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a19740",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
